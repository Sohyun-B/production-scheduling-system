"""
Level 4 ì „ëµ í…ŒìŠ¤íŠ¸ ë° ê²°ê³¼ ìƒì„±

Level 4 DispatchPriorityStrategyë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤ì¼€ì¤„ë§ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê³ 
main.ipynbì™€ ë™ì¼í•œ í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
"""

import warnings
warnings.filterwarnings('ignore', category=FutureWarning)

import pandas as pd
from datetime import datetime
import sys
import os
import json

from config import config
from src.preprocessing import preprocessing
from src.yield_management import yield_prediction
from src.dag_management import create_complete_dag_system
from src.scheduler.scheduling_core import DispatchPriorityStrategy
from src.results import create_results

def run_level4_scheduling():

    base_date = datetime(config.constants.BASE_YEAR, config.constants.BASE_MONTH, config.constants.BASE_DAY)
    window_days = config.constants.WINDOW_DAYS

    # === 1ë‹¨ê³„: JSON ë°ì´í„° ë¡œë”© ===
    print("[10%] JSON ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # JSON íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° ë¡œë”©
    try:
        print("JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # 1. í’ˆëª©ë³„ ë¼ì¸ìŠ¤í”¼ë“œ ë° ê³µì • ìˆœì„œ ê´€ë ¨
        linespeed = pd.read_json(config.files.JSON_LINESPEED)
        operation_seperated_sequence = pd.read_json(config.files.JSON_OPERATION_SEQUENCE)
        machine_master_info = pd.read_json(config.files.JSON_MACHINE_INFO)
        yield_data = pd.read_json(config.files.JSON_YIELD_DATA)
        gitem_operation = pd.read_json(config.files.JSON_GITEM_OPERATION)
        
        print(f"[ë°ì´í„°] ë¼ì¸ìŠ¤í”¼ë“œ {len(linespeed)}ê°œ, ê¸°ê³„ì •ë³´ {len(machine_master_info)}ê°œ")
        
        # 2. ê³µì • ì¬ë¶„ë¥˜ ë‚´ì—­ ë° êµì²´ ì‹œê°„ ê´€ë ¨
        operation_types = pd.read_json(config.files.JSON_OPERATION_TYPES)
        operation_delay_df = pd.read_json(config.files.JSON_OPERATION_DELAY)
        width_change_df = pd.read_json(config.files.JSON_WIDTH_CHANGE)
        
        print(f"[ë°ì´í„°] ê³µì •ë¶„ë¥˜ {len(operation_types)}ê°œ, ì§€ì—°ì •ë³´ {len(operation_delay_df)}ê°œ")
        
        # 3. ë¶ˆê°€ëŠ¥í•œ ê³µì • ì…ë ¥ê°’ ê´€ë ¨ (ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜ í•„ìš”)
        machine_rest = pd.read_json(config.files.JSON_MACHINE_REST)
        # machine_restì˜ ë‚ ì§œ ì»¬ëŸ¼ë“¤ì„ datetimeìœ¼ë¡œ ë³€í™˜
        if 'ì‹œì‘ì‹œê°„' in machine_rest.columns:
            machine_rest['ì‹œì‘ì‹œê°„'] = pd.to_datetime(machine_rest['ì‹œì‘ì‹œê°„'])
        if 'ì¢…ë£Œì‹œê°„' in machine_rest.columns:
            machine_rest['ì¢…ë£Œì‹œê°„'] = pd.to_datetime(machine_rest['ì¢…ë£Œì‹œê°„'])
        
        machine_allocate = pd.read_json(config.files.JSON_MACHINE_ALLOCATE)
        machine_limit = pd.read_json(config.files.JSON_MACHINE_LIMIT)
        
        print(f"[ë°ì´í„°] ê¸°ê³„í• ë‹¹ {len(machine_allocate)}ê°œ, ê¸°ê³„ì œí•œ {len(machine_limit)}ê°œ")
        
        # 4. ì£¼ë¬¸ ë°ì´í„° (ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜ í•„ìš”)
        order = pd.read_json(config.files.JSON_ORDER_DATA)
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        if config.columns.DUE_DATE in order.columns:
            order[config.columns.DUE_DATE] = pd.to_datetime(order[config.columns.DUE_DATE])
        
        print(f"[ì£¼ë¬¸] ì´ {len(order)}ê°œ ì£¼ë¬¸ ë¡œë”© ì™„ë£Œ")
        
    except FileNotFoundError as e:
        print(f"ì˜¤ë¥˜: {e}")

    # # === 1ë‹¨ê³„ ì™„ë£Œ: JSON ì €ì¥ ===
    # DB-ë°±ì—”ë“œì—ì„œ ë°”ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½ 
    # import json
    # stage1_data = {
    #     "stage": "loading",
    #     "data": {
    #         "linespeed_count": len(linespeed),
    #         "machine_count": len(machine_master_info),
    #         "operation_types_count": len(operation_types),
    #         "operation_delay_count": len(operation_delay_df),
    #         "total_orders": len(order),
    #         "base_config": {
    #             "base_year": config.constants.BASE_YEAR,
    #             "base_month": config.constants.BASE_MONTH,
    #             "base_day": config.constants.BASE_DAY,
    #             "window_days": window_days
    #         }
    #     }
    # }
    
    # with open("data/output/stage1_loading.json", "w", encoding="utf-8") as f:
    #     json.dump(stage1_data, f, ensure_ascii=False, default=str)
    # print("[ë‹¨ê³„1] JSON ì €ì¥ ì™„ë£Œ: data/output/stage1_loading.json")


    # 1ë‹¨ê³„: VALIDATION

    # === 2ë‹¨ê³„: ì „ì²˜ë¦¬ ===
    print("[30%] ì£¼ë¬¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    sequence_seperated_order, linespeed = preprocessing(order, operation_seperated_sequence, operation_types, machine_limit, machine_allocate, linespeed)
    print(f"[ì „ì²˜ë¦¬] ì „ì²˜ë¦¬ ì™„ë£Œ: {len(sequence_seperated_order)}ê°œ ì‘ì—… ìƒì„±")
    
    # === 2ë‹¨ê³„ ì™„ë£Œ: JSON ì €ì¥ ===
    stage2_data = {
        "stage": "preprocessing", 
        "data": {
            "input_orders": len(order),
            "processed_jobs": len(sequence_seperated_order),
            "machine_constraints": {
                "machine_rest": machine_rest.to_dict('records'),
                "machine_allocate": machine_allocate.to_dict('records'),
                "machine_limit": machine_limit.to_dict('records')
            }
        }
    }
    
    with open("data/output/stage2_preprocessing.json", "w", encoding="utf-8") as f:
        json.dump(stage2_data, f, ensure_ascii=False, default=str)
    print("[ë‹¨ê³„2] JSON ì €ì¥ ì™„ë£Œ: data/output/stage2_preprocessing.json")
    
    # === 3ë‹¨ê³„: ìˆ˜ìœ¨ ì˜ˆì¸¡ (3ë‹¨ê³„, 4ë‹¨ê³„ ê±´ë„ˆë›°ê¸°) ===
    print("[35%] ìˆ˜ìœ¨ ì˜ˆì¸¡ ì²˜ë¦¬ ì¤‘...")
    yield_predictor, sequence_yield_df, sequence_seperated_order = yield_prediction(
        yield_data, gitem_operation, sequence_seperated_order
    )
    
    # === 4ë‹¨ê³„: DAG ìƒì„± ===
    print("[40%] DAG ì‹œìŠ¤í…œ ìƒì„± ì¤‘...")
    dag_df, opnode_dict, manager, machine_dict, merged_df = create_complete_dag_system(
        sequence_seperated_order, linespeed, machine_master_info, config
    )
    print(f"[50%] DAG ì‹œìŠ¤í…œ ìƒì„± ì™„ë£Œ - ë…¸ë“œ: {len(dag_df)}ê°œ, ê¸°ê³„: {len(machine_dict)}ê°œ")
    
    # === 5ë‹¨ê³„: ìŠ¤ì¼€ì¤„ë§ ì‹¤í–‰ ===
    print("[60%] ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ ì´ˆê¸°í™” ì¤‘...")
    try:
        # ìŠ¤ì¼€ì¤„ë§ ì¤€ë¹„
        from src.scheduler.delay_dict import DelayProcessor
        from src.scheduler.scheduler import Scheduler
        from src.scheduler.dispatch_rules import create_dispatch_rule
        
        # ë””ìŠ¤íŒ¨ì¹˜ ë£° ìƒì„±
        print("[65%] ë””ìŠ¤íŒ¨ì¹˜ ê·œì¹™ ìƒì„± ì¤‘...")
        dispatch_rule_ans, dag_df = create_dispatch_rule(dag_df, sequence_seperated_order)
        print(f"[ë””ìŠ¤íŒ¨ì¹˜] ìš°ì„ ìˆœìœ„ ê·œì¹™ ìƒì„± ì™„ë£Œ: {len(dispatch_rule_ans)}ê°œ ê·œì¹™")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        print("[70%] ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ë° ìì› í• ë‹¹ ì¤‘...")
        delay_processor = DelayProcessor(opnode_dict, operation_delay_df, width_change_df)
        scheduler = Scheduler(machine_dict, delay_processor)
        scheduler.allocate_resources()
        scheduler.allocate_machine_downtime(machine_rest, base_date)
        print("[ìŠ¤ì¼€ì¤„ëŸ¬] ê¸°ê³„ ìì› í• ë‹¹ ì™„ë£Œ, ê¸°ê³„ ì¤‘ë‹¨ì‹œê°„ ì„¤ì • ì™„ë£Œ")
        
        # ì „ëµ ì‹¤í–‰ (ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë‹¨ê³„)
        print("[75%] ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì¤‘...")
        strategy = DispatchPriorityStrategy()
        result = strategy.execute(
            dag_manager=manager,
            scheduler=scheduler,
            dag_df=dag_df,
            priority_order=dispatch_rule_ans,
            window_days=window_days
        )
        print("[85%] ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì™„ë£Œ!")
        
        # depth -1ì¸ ê°€ì§œ ê³µì • ì œì™¸í•œ ì‹¤ì œ makespan ê³„ì‚°
        actual_makespan = result[~(result['depth'] == -1)]['node_end'].max()
        total_makespan = result['node_end'].max()
        
        # ê¸°ê³„ ìŠ¤ì¼€ì¤„ ì •ë³´ ìƒì„± (stage5ìš©)
        machine_schedule_df_stage5 = scheduler.create_machine_schedule_dataframe()
        # í• ë‹¹ ì‘ì—…ì´ -1 ê³µì •ì¸ ê²½ìš° ì‚­ì œ
        machine_schedule_df_stage5 = machine_schedule_df_stage5[~machine_schedule_df_stage5['í• ë‹¹ ì‘ì—…'].astype(str).str.startswith('[-1', na=False)]
        
        # === 5ë‹¨ê³„ ì™„ë£Œ: JSON ì €ì¥ (machine_info í¬í•¨) ===
        stage5_data = {
            "stage": "scheduling",
            "data": {
                "window_days_used": window_days,
                "makespan_slots": int(actual_makespan),
                "makespan_hours": actual_makespan * 0.5,
                "total_days": (actual_makespan * 0.5) / 24,
                "processed_jobs_count": len(result[~(result['depth'] == -1)]),
                "machine_info": machine_schedule_df_stage5.to_dict('records')
            }
        }
        
        with open("data/output/stage5_scheduling.json", "w", encoding="utf-8") as f:
            json.dump(stage5_data, f, ensure_ascii=False, default=str)
        print("[ë‹¨ê³„5] JSON ì €ì¥ ì™„ë£Œ (machine_info í¬í•¨): data/output/stage5_scheduling.json")
        
        # === 6ë‹¨ê³„: ê²°ê³¼ í›„ì²˜ë¦¬ ===
        print(f"[90%] ìŠ¤ì¼€ì¤„ë§ ì™„ë£Œ! Makespan: {actual_makespan:.1f} (ì´ {actual_makespan/48:.1f}ì¼)")
        print(f"[ê²°ê³¼] ì‹¤ì œ Makespan: {actual_makespan} (ê°€ì§œ ê³µì • ì œì™¸)")
        print(f"[ê²°ê³¼] ì „ì²´ Makespan: {total_makespan} (ê°€ì§œ ê³µì • í¬í•¨)")
        print(f"[ê²°ê³¼] ì´ ì†Œìš”ì‹œê°„: {actual_makespan / 48:.2f}ì¼")
        
        # ì›ë³¸ ê²°ê³¼ ì €ì¥
        print("[92%] ì›ë³¸ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì¤‘...")
        excel_filename = "data/output/result.xlsx"
        result.to_excel(excel_filename, index=False)
        print(f"[ì €ì¥] ì›ë³¸ ê²°ê³¼ë¥¼ '{excel_filename}'ì— ì €ì¥ ì™„ë£Œ")
        
        # ê°€ê³µëœ ê²°ê³¼ ìƒì„± ë° ì €ì¥
        # depth -1ì¸ ê°€ì§œ ê³µì • ì‚­ì œ
        result_cleaned = result[~(result['depth'] == -1)]
        
        # ê¸°ê³„ ìŠ¤ì¼€ì¤„ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        machine_schedule_df = scheduler.create_machine_schedule_dataframe()
        # í• ë‹¹ ì‘ì—…ì´ -1 ê³µì •ì¸ ê²½ìš° ì‚­ì œ
        machine_schedule_df = machine_schedule_df[~machine_schedule_df['í• ë‹¹ ì‘ì—…'].astype(str).str.startswith('[-1', na=False)]
        
        
        # create_results í•¨ìˆ˜ í˜¸ì¶œ
        results = create_results(
            output_final_result=result_cleaned,
            merged_df=merged_df,
            original_order=order,
            sequence_seperated_order=sequence_seperated_order,
            machine_mapping=machine_master_info.set_index('ê¸°ê³„ì¸ë±ìŠ¤')['ê¸°ê³„ì½”ë“œ'].to_dict(),
            machine_schedule_df=machine_schedule_df,
            base_date=base_date,
            scheduler=scheduler
        )
        
        # ê²°ê³¼ ë¶„ì„
        print(f"[ë¶„ì„] ì§€ê° ì´ ì¼ ìˆ˜: {results['late_days_sum']}")
        print(f"[ë¶„ì„] ì´ ì†Œìš”ì‹œê°„: {results['new_output_final_result']['ì¢…ë£Œì‹œê°'].max() / 48}ì¼")
        print(f"[ë¶„ì„] ì´ makespan: {results['new_output_final_result']['ì¢…ë£Œì‹œê°'].max()}")
        
        # ë°ì´í„° ê²€ì¦
        print(f"[ê²€ì¦] ì›ë³¸ ê²°ê³¼ í–‰ ìˆ˜: {len(result_cleaned)}")
        print(f"[ê²€ì¦] ì²˜ë¦¬ëœ ê²°ê³¼ í–‰ ìˆ˜: {len(results['new_output_final_result'])}")
        print(f"[ê²€ì¦] ì›ë³¸ ìµœëŒ€ node_end: {result_cleaned['node_end'].max()}")
        print(f"[ê²€ì¦] ì²˜ë¦¬ëœ ìµœëŒ€ ì¢…ë£Œì‹œê°: {results['new_output_final_result']['ì¢…ë£Œì‹œê°'].max()}")
        
        # GITEMëª… ì •ë³´ëŠ” ì´ë¯¸ ì „ì²˜ë¦¬ëœ order ë°ì´í„°ì— ìˆìŒ
        order_with_names = order[['GITEM', 'GITEMëª…']].drop_duplicates()
        
        # main.ipynbì™€ ë™ì¼í•œ í›„ì²˜ë¦¬ ê³¼ì •
        order_summary = results['new_output_final_result'].copy()
        machine_info = results['machine_info'].copy()
        machine_info.to_csv("ì´ê±¸ë¡œ ê°„íŠ¸ë§Œë“¤ì–´ì•¼í•¨.csv", encoding='utf-8-sig')
        order_info = results['merged_result'].copy()
        
        # 1. order_infoì—ì„œ ì¡°í•©ë¶„ë¥˜ë¡œ ì‹œì‘í•˜ëŠ” ì»¬ëŸ¼ ì „ë¶€ ì‚­ì œ
        order_info = order_info.loc[:, ~order_info.columns.str.startswith("ì¡°í•©ë¶„ë¥˜")]
        
        # 2. order_summary : ì£¼ë¬¸ ìƒì‚° ìš”ì•½ë³¸ìœ¼ë¡œ ì´ë¦„ ë³€ê²½, ì¢…ë£Œì‹œê° ì‚­ì œ
        if 'ì¢…ë£Œì‹œê°' in order_summary.columns:
            order_summary = order_summary[['P/O NO', '1ê³µì •ID', '2ê³µì •ID', '3ê³µì •ID', '4ê³µì •ID', 'GITEM', 'ë‚©ê¸°ì¼', 'ì¢…ë£Œë‚ ì§œ', 'ì§€ê°ì¼ìˆ˜']]
        
        # 3. machine_info ì²˜ë¦¬
        # ê¸°ê³„ì¸ë±ìŠ¤ë¥¼ ê¸°ê³„ì½”ë“œë¡œ ë§¤í•‘
        index_to_code_mapping = machine_master_info.set_index('ê¸°ê³„ì¸ë±ìŠ¤')['ê¸°ê³„ì½”ë“œ'].to_dict()
        code_to_name_mapping = machine_master_info.set_index('ê¸°ê³„ì½”ë“œ')['ê¸°ê³„ì´ë¦„'].to_dict()
        
        machine_info = machine_info.rename(columns = {"ê¸°ê³„ì¸ë±ìŠ¤": "ê¸°ê³„ì½”ë“œ"})
        machine_info['ê¸°ê³„ì´ë¦„'] = machine_info['ê¸°ê³„ì½”ë“œ'].map(code_to_name_mapping)
        
        # GITEMëª… ì¶”ê°€
        machine_info = pd.merge(machine_info, order_with_names, on='GITEM', how='left')
        
        # ì¶”ê°€ ì»¬ëŸ¼ ìƒì„±
        machine_info['ê³µì •ëª…'] = machine_info['ID'].str.split('_').str[1]
        machine_info['ì‘ì—…ì‹œê°„'] = machine_info['ì‘ì—… ì¢…ë£Œ ì‹œê°„'] - machine_info['ì‘ì—… ì‹œì‘ ì‹œê°„']
        
        # ìµœì¢… ì—‘ì…€ íŒŒì¼ ì €ì¥ (main.ipynbì™€ ë™ì¼í•œ í˜•íƒœ)
        print("[95%] ìµœì¢… Excel íŒŒì¼ ì €ì¥ ì¤‘...")
        processed_filename = "data/output/0829 ìŠ¤ì¼€ì¤„ë§ê²°ê³¼.xlsx"
        with pd.ExcelWriter(processed_filename, engine="openpyxl") as writer:
            order_summary.to_excel(writer, sheet_name="ì£¼ë¬¸_ìƒì‚°_ìš”ì•½ë³¸", index=False)
            order_info.to_excel(writer, sheet_name="ì£¼ë¬¸_ìƒì‚°_ì •ë³´", index=False)
            machine_info.to_excel(writer, sheet_name="í˜¸ê¸°_ì •ë³´", index=False)
        
        print(f"[ì €ì¥] ê°€ê³µëœ ê²°ê³¼ë¥¼ '{processed_filename}'ì— ì €ì¥ ì™„ë£Œ")
        
        # === ê°„ê²© ë¶„ì„ ì¶”ê°€ ===
        print("[96%] ìŠ¤ì¼€ì¤„ ê°„ê²© ë¶„ì„ ì¤‘...")
        try:
            from src.results.gap_analyzer import ScheduleGapAnalyzer
            
            # ê°„ê²© ë¶„ì„ê¸° ìƒì„±
            gap_analyzer = ScheduleGapAnalyzer(scheduler, delay_processor)
            
            # ê¸°ê³„ë³„ ìŠ¤ì¼€ì¤„ ê²°ê³¼ ì²˜ë¦¬ê¸°ì— ê°„ê²© ë¶„ì„ê¸° ì „ë‹¬
            from src.results.machine_schedule import MachineScheduleProcessor
            processor = MachineScheduleProcessor(
                machine_master_info.set_index('ê¸°ê³„ì¸ë±ìŠ¤')['ê¸°ê³„ì½”ë“œ'].to_dict(),
                machine_schedule_df,
                result_cleaned,
                base_date,
                gap_analyzer
            )
            
            # ê°„ê²© ë¶„ì„ ìš”ì•½ ì¶œë ¥
            processor.print_gap_summary()
            
            # ìƒì„¸ ê°„ê²© ë¶„ì„ ê²°ê³¼ ì €ì¥
            detailed_gaps, machine_summary = processor.create_gap_analysis_report()
            if detailed_gaps is not None:
                detailed_gaps.to_excel("data/output/schedule_gaps_detailed.xlsx", index=False)
                print("[ê°„ê²©ë¶„ì„] ìƒì„¸ ê°„ê²© ë¶„ì„ ê²°ê³¼ ì €ì¥: schedule_gaps_detailed.xlsx")
            
            if machine_summary is not None:
                machine_summary.to_excel("data/output/machine_gap_summary.xlsx", index=False)
                print("[ê°„ê²©ë¶„ì„] ê¸°ê³„ë³„ ê°„ê²© ìš”ì•½ ì €ì¥: machine_gap_summary.xlsx")
            
        except Exception as gap_error:
            print(f"[WARNING] ê°„ê²© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {gap_error}")
            gap_analyzer = None

        # ê°„íŠ¸ ì°¨íŠ¸ ìƒì„± ë° ì €ì¥ (ê°„ê²© ì •ë³´ í¬í•¨)
        print("[97%] ê°„íŠ¸ ì°¨íŠ¸ ìƒì„± ì¤‘...")
        gantt_filename = "data/output/level4_gantt.png"
        try:
            from src.scheduler.chart import DrawChart
            
            # ê°„ê²© ë¶„ì„ê¸°ê°€ ìˆìœ¼ë©´ í¬í•¨í•˜ì—¬ ì°¨íŠ¸ ìƒì„±
            gantt = DrawChart(scheduler.Machines, gap_analyzer)
            gantt_plot = gantt.plot(show_gaps=True if gap_analyzer else False)
            
            # matplotlib ë°±ì—”ë“œë¥¼ Aggë¡œ ì„¤ì •í•˜ì—¬ ì°½ì´ ëœ¨ì§€ ì•Šë„ë¡ í•¨
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            
            if hasattr(gantt_plot, 'savefig'):
                gantt_plot.savefig(gantt_filename, dpi=300, bbox_inches='tight')
                plt.close('all')  # ëª¨ë“  figure ë‹«ê¸°
                print(f"[ì°¨íŠ¸] Gantt chartë¥¼ '{gantt_filename}'ì— ì €ì¥ ì™„ë£Œ")
                
            elif hasattr(gantt_plot, 'write_image'):
                gantt_plot.write_image(gantt_filename)
                print(f"[ì°¨íŠ¸] Gantt chartë¥¼ '{gantt_filename}'ì— ì €ì¥ ì™„ë£Œ")
                
            else:
                print(f"[ì°¨íŠ¸] ê°„íŠ¸ ì°¨íŠ¸ íƒ€ì…: {type(gantt_plot)}")
                
            # íŒŒì¼ ìƒì„± í™•ì¸
            if os.path.exists(gantt_filename):
                file_size = os.path.getsize(gantt_filename)
                print(f"[ì°¨íŠ¸] ê°„íŠ¸ ì°¨íŠ¸ íŒŒì¼: {gantt_filename} ({file_size} bytes)")
                if gap_analyzer:
                    print("[ì°¨íŠ¸] ğŸ“ ë¹¨ê°„ìƒ‰: ì…‹ì—…ì‹œê°„, íšŒìƒ‰: ëŒ€ê¸°ì‹œê°„ìœ¼ë¡œ í‘œì‹œë¨")
                    
        except Exception as chart_error:
            print(f"[ERROR] ê°„íŠ¸ ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {chart_error}")
        
        # ì§€ê°í•œ ì œí’ˆ ì •ë³´ ì¶”ì¶œ
        late_products = results['new_output_final_result'][results['new_output_final_result']['ì§€ê°ì¼ìˆ˜'] > 0]
        late_po_numbers = late_products['P/O NO'].tolist() if len(late_products) > 0 else []
        
        # === 6ë‹¨ê³„ ì™„ë£Œ: JSON ì €ì¥ (ì§€ê° ì •ë³´ë§Œ) ===
        stage6_data = {
            "stage": "results",
            "data": {
                "late_days_sum": results['late_days_sum'],
                "late_products_count": len(late_products),
                "late_po_numbers": late_po_numbers
            }
        }
        
        with open("data/output/stage6_results.json", "w", encoding="utf-8") as f:
            json.dump(stage6_data, f, ensure_ascii=False, default=str)
        print("[ë‹¨ê³„6] JSON ì €ì¥ ì™„ë£Œ: data/output/stage6_results.json")
        
        # ìµœì¢… ì™„ë£Œ
        print("[100%] ìŠ¤ì¼€ì¤„ë§ ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        print(f"[ERROR] Level 4 ìŠ¤ì¼€ì¤„ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return

if __name__ == "__main__":
    run_level4_scheduling()